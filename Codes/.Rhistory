sig.m = runif(K,0,1)
pi.s = pi.m
mu.s = runif(K,0,1)
sig.s = runif(K,0,0.1)
Z = GenerateZ(n, q, pi.m, mu.m, sig.m, pi.s, mu.s, sig.s)
iter = 0; CONVERGE=FALSE; refit.B=TRUE; update.counter=0;
updateTheta = FALSE; # we don't update Theta until B is stabilized a bit
B_new = B_init
Omega_new = Omega_init
Normdiff = rep(0, maxit)
Objval = rep(0, maxit)
B.list = list()
Omega.list = list()
var.list = list()
iter = iter + 1
B_old = B_new
Omega_old = Omega_new
Omega.skeleton = which(Omega_old!=0, arr.ind=T)
cat("Iteration ", iter, ":\n")
## Update variational parameters using SGD
cat("Updating variational parameters\n")
epoch = 0
CONVERGE.var=FALSE
while(!CONVERGE.var){
epoch = epoch+1
eta.epoch = eta*exp(-epoch+1)
perm = gtools::permute(1:n) # randomly order samples
pi.m.old = pi.m
pi.s.old = pi.s
mu.m.old = mu.m
mu.s.old = mu.s
sig.m.old = sig.m
sig.s.old = sig.s
## terms corresp to squared error loss
for(i in perm){ # SGD: go through all samples randomly
xi = X[i,]
for(j0 in 1:nrow(Omega.skeleton)){
# calculate quantities
j = Omega.skeleton[j0,1]
j1 = Omega.skeleton[j0,2]
xj.1.bj1 = xi[j]*sum(B_old[j1,])
bj.t.bj1 = sum(B_old[j,]*B_old[j1,])
mult1 = -(2*Omega_old[j,j1]*xj.1.bj1)/n
# mult2 = exp(clip(2*(mu.s.old + sig.s.old^2), ceil=3))*bj.t.bj1
mult2 = exp(-2*(mu.s + sig.s^2))
# mult2 = exp(2*(mu.s.old + sig.s.old^2))*(bj.t.bj1 +
#                                            q/2/nrow(Omega.skeleton))
# calculatre gradients
grad.pi.m = mult1*mu.m + (mu.m^2+sig.m^2)*bj.t.bj1
grad.mu.m = pi.m*(mult1 + 2*mu.m*bj.t.bj1)
grad.sig.m = 2*sig.m*pi.m*bj.t.bj1
grad.pi.s = mult2*bj.t.bj1
grad.mu.s = -2*pi.s*mult2*bj.t.bj1
grad.sig.s = -4*pi.s*sig.s*mult2*bj.t.bj1
## terms corresp to first KL divergence
mult3 = mult2/2
grad1.pi.m = (mu.m^2+sig.m^2)/2
grad1.mu.m = pi.m*mu.m
grad1.sig.m = pi.m*sig.m
grad1.pi.s = mult3 + mu.s
grad1.mu.s = -2*pi.s.old*mult3 + 1
grad1.sig.s = -4*pi.s.old*sig.s.old*mult3
## terms corresp. to second KL divergence
# update parameters of r(.)
phi.m.sq = clip(sum(pi.m/sig.m^2), ceil=1e3)
phi.s.sq = clip(sum(pi.s/sig.s^2), ceil=1e3)
# calculate gradients
sumZi2 = sum(Z[i,]^2); sumZi = sum(Z[i,])
grad2.pi.m = (sumZi2 - 2*sumZi*mu.m +
q*(mu.m^2 + phi.m.sq))/2/n/sig.m^2
grad2.mu.m = pi.m*(q*mu.m - sumZi)/n/sig.m^2
grad2.sig.m = -pi.m*(sumZi2 - 2*sumZi*mu.m +
q*(mu.m^2 + phi.m.sq))/n/sig.m^3
grad2.pi.s = (sumZi2 - 2*sumZi*mu.s +
q*(mu.s^2 + phi.s.sq))/2/n/sig.s^2
grad2.mu.s = pi.s*(q*mu.s - sumZi)/n/sig.s^2
grad2.sig.s = -pi.s*(sumZi2 - 2*sumZi*mu.s +
q*(mu.s^2 + phi.s.sq))/n/sig.s^3
# update
pi.m = pi.m - eta.epoch*clip(grad.pi.m+grad1.pi.m+grad2.pi.m)
mu.m = mu.m - eta.epoch*clip(grad.mu.m+grad1.mu.m+grad2.mu.m)
sig.m = sig.m - eta.epoch*clip(grad.sig.m+grad1.sig.m+grad2.sig.m)
pi.s = pi.s - eta.epoch*clip(grad.pi.s+grad1.pi.s+grad2.pi.s)
mu.s = mu.s - eta.epoch*clip(grad.mu.s+grad1.mu.s+grad2.mu.s)
sig.s = sig.s - eta.epoch*clip(grad.sig.s+grad1.sig.s+grad2.sig.s)
# normalize parameters
pi.m = abs(pi.m)/sum(abs(pi.m))
pi.s = abs(pi.s)/sum(abs(pi.s))
sig.m = abs(sig.m)
sig.s = abs(sig.s)
}
}
# cat('\t',sig.s,'\n----------\n')
# check convergence
var.diff = sqrt(sum((pi.m.old-pi.m)^2)/sum(pi.m^2)) +
sqrt(sum((pi.s.old-pi.s)^2)/sum(pi.s^2)) +
sqrt(sum((mu.m.old-mu.m)^2)/sum(mu.m^2)) +
sqrt(sum((mu.s.old-mu.s)^2)/sum(mu.s^2)) +
sqrt(sum((sig.m.old-sig.m)^2)/sum(sig.m^2)) +
sqrt(sum((sig.s.old-sig.s)^2)/sum(sig.s^2))
cat("Norm_diff =",round(var.diff,4),sig.s,'\n-----\n')
CONVERGE.var = (var.diff<tol)
if(epoch==max.epoch){
cat("Max iterations reached.",'\n')
break;
}
}
set.seed(07232018)
pi.m = rep(1/K,K)
mu.m = runif(K,-1,1)
sig.m = runif(K,0,1)
pi.s = pi.m
mu.s = runif(K,0,1)
sig.s = runif(K,0,0.1)
Z = GenerateZ(n, q, pi.m, mu.m, sig.m, pi.s, mu.s, sig.s)
iter = 0; CONVERGE=FALSE; refit.B=TRUE; update.counter=0;
updateTheta = FALSE; # we don't update Theta until B is stabilized a bit
B_new = B_init
Omega_new = Omega_init
Normdiff = rep(0, maxit)
Objval = rep(0, maxit)
B.list = list()
Omega.list = list()
var.list = list()
iter = iter + 1
B_old = B_new
Omega_old = Omega_new
Omega.skeleton = which(Omega_old!=0, arr.ind=T)
cat("Iteration ", iter, ":\n")
## Update variational parameters using SGD
cat("Updating variational parameters\n")
epoch = 0
CONVERGE.var=FALSE
while(!CONVERGE.var){
epoch = epoch+1
eta.epoch = eta*exp(-epoch+1)
perm = gtools::permute(1:n) # randomly order samples
pi.m.old = pi.m
pi.s.old = pi.s
mu.m.old = mu.m
mu.s.old = mu.s
sig.m.old = sig.m
sig.s.old = sig.s
## terms corresp to squared error loss
for(i in perm){ # SGD: go through all samples randomly
xi = X[i,]
for(j0 in 1:nrow(Omega.skeleton)){
# calculate quantities
j = Omega.skeleton[j0,1]
j1 = Omega.skeleton[j0,2]
xj.1.bj1 = xi[j]*sum(B_old[j1,])
bj.t.bj1 = sum(B_old[j,]*B_old[j1,])
mult1 = -(2*Omega_old[j,j1]*xj.1.bj1)/n
# mult2 = exp(clip(2*(mu.s.old + sig.s.old^2), ceil=3))*bj.t.bj1
mult2 = exp(-2*(mu.s + sig.s^2))
# mult2 = exp(2*(mu.s.old + sig.s.old^2))*(bj.t.bj1 +
#                                            q/2/nrow(Omega.skeleton))
# calculatre gradients
grad.pi.m = mult1*mu.m + (mu.m^2+sig.m^2)*bj.t.bj1
grad.mu.m = pi.m*(mult1 + 2*mu.m*bj.t.bj1)
grad.sig.m = 2*sig.m*pi.m*bj.t.bj1
grad.pi.s = mult2*bj.t.bj1
grad.mu.s = -2*pi.s*mult2*bj.t.bj1
grad.sig.s = -4*pi.s*sig.s*mult2*bj.t.bj1
## terms corresp to first KL divergence
mult3 = mult2/2*q/n
grad1.pi.m = (mu.m^2+sig.m^2)/2*q/n
grad1.mu.m = pi.m*mu.m*q/n
grad1.sig.m = pi.m*sig.m*q/n
grad1.pi.s = mult3 + mu.s*q/n
grad1.mu.s = -2*pi.s.old*mult3 + 1*q/n
grad1.sig.s = -4*pi.s.old*sig.s.old*mult3
## terms corresp. to second KL divergence
# update parameters of r(.)
phi.m.sq = clip(sum(pi.m/sig.m^2), ceil=1e3)
phi.s.sq = clip(sum(pi.s/sig.s^2), ceil=1e3)
# calculate gradients
sumZi2 = sum(Z[i,]^2); sumZi = sum(Z[i,])
grad2.pi.m = (sumZi2 - 2*sumZi*mu.m +
q*(mu.m^2 + phi.m.sq))/2/n/sig.m^2
grad2.mu.m = pi.m*(q*mu.m - sumZi)/n/sig.m^2
grad2.sig.m = -pi.m*(sumZi2 - 2*sumZi*mu.m +
q*(mu.m^2 + phi.m.sq))/n/sig.m^3
grad2.pi.s = (sumZi2 - 2*sumZi*mu.s +
q*(mu.s^2 + phi.s.sq))/2/n/sig.s^2
grad2.mu.s = pi.s*(q*mu.s - sumZi)/n/sig.s^2
grad2.sig.s = -pi.s*(sumZi2 - 2*sumZi*mu.s +
q*(mu.s^2 + phi.s.sq))/n/sig.s^3
# update
pi.m = pi.m - eta.epoch*clip(grad.pi.m+grad1.pi.m+grad2.pi.m)
mu.m = mu.m - eta.epoch*clip(grad.mu.m+grad1.mu.m+grad2.mu.m)
sig.m = sig.m - eta.epoch*clip(grad.sig.m+grad1.sig.m+grad2.sig.m)
pi.s = pi.s - eta.epoch*clip(grad.pi.s+grad1.pi.s+grad2.pi.s)
mu.s = mu.s - eta.epoch*clip(grad.mu.s+grad1.mu.s+grad2.mu.s)
sig.s = sig.s - eta.epoch*clip(grad.sig.s+grad1.sig.s+grad2.sig.s)
# normalize parameters
pi.m = abs(pi.m)/sum(abs(pi.m))
pi.s = abs(pi.s)/sum(abs(pi.s))
sig.m = abs(sig.m)
sig.s = abs(sig.s)
}
}
# cat('\t',sig.s,'\n----------\n')
# check convergence
var.diff = sqrt(sum((pi.m.old-pi.m)^2)/sum(pi.m^2)) +
sqrt(sum((pi.s.old-pi.s)^2)/sum(pi.s^2)) +
sqrt(sum((mu.m.old-mu.m)^2)/sum(mu.m^2)) +
sqrt(sum((mu.s.old-mu.s)^2)/sum(mu.s^2)) +
sqrt(sum((sig.m.old-sig.m)^2)/sum(sig.m^2)) +
sqrt(sum((sig.s.old-sig.s)^2)/sum(sig.s^2))
cat("Norm_diff =",round(var.diff,4),sig.s,'\n-----\n')
CONVERGE.var = (var.diff<tol)
if(epoch==max.epoch){
cat("Max iterations reached.",'\n')
break;
}
}
set.seed(07232018)
pi.m = rep(1/K,K)
mu.m = runif(K,-1,1)
sig.m = runif(K,0,1)
pi.s = pi.m
mu.s = runif(K,0,1)
sig.s = runif(K,0,0.1)
Z = GenerateZ(n, q, pi.m, mu.m, sig.m, pi.s, mu.s, sig.s)
iter = 0; CONVERGE=FALSE; refit.B=TRUE; update.counter=0;
updateTheta = FALSE; # we don't update Theta until B is stabilized a bit
B_new = B_init
Omega_new = Omega_init
Normdiff = rep(0, maxit)
Objval = rep(0, maxit)
B.list = list()
Omega.list = list()
var.list = list()
iter = iter + 1
B_old = B_new
Omega_old = Omega_new
Omega.skeleton = which(Omega_old!=0, arr.ind=T)
cat("Iteration ", iter, ":\n")
## Update variational parameters using SGD
cat("Updating variational parameters\n")
epoch = 0
CONVERGE.var=FALSE
while(!CONVERGE.var){
epoch = epoch+1
eta.epoch = eta*exp(-epoch+1)
perm = gtools::permute(1:n) # randomly order samples
pi.m.old = pi.m
pi.s.old = pi.s
mu.m.old = mu.m
mu.s.old = mu.s
sig.m.old = sig.m
sig.s.old = sig.s
## terms corresp to squared error loss
for(i in perm){ # SGD: go through all samples randomly
xi = X[i,]
for(j0 in 1:nrow(Omega.skeleton)){
# calculate quantities
j = Omega.skeleton[j0,1]
j1 = Omega.skeleton[j0,2]
xj.1.bj1 = xi[j]*sum(B_old[j1,])
bj.t.bj1 = sum(B_old[j,]*B_old[j1,])
mult1 = -(2*Omega_old[j,j1]*xj.1.bj1)/n
# mult2 = exp(clip(2*(mu.s.old + sig.s.old^2), ceil=3))*bj.t.bj1
mult2 = exp(-2*(mu.s + sig.s^2))
# mult2 = exp(2*(mu.s.old + sig.s.old^2))*(bj.t.bj1 +
#                                            q/2/nrow(Omega.skeleton))
# calculatre gradients
grad.pi.m = mult1*mu.m + (mu.m^2+sig.m^2)*bj.t.bj1
grad.mu.m = pi.m*(mult1 + 2*mu.m*bj.t.bj1)
grad.sig.m = 2*sig.m*pi.m*bj.t.bj1
grad.pi.s = mult2*bj.t.bj1
grad.mu.s = -2*pi.s*mult2*bj.t.bj1
grad.sig.s = -4*pi.s*sig.s*mult2*bj.t.bj1
## terms corresp to first KL divergence
mult3 = mult2/2*q/n
grad1.pi.m = (mu.m^2+sig.m^2)/2/q/n
grad1.mu.m = pi.m*mu.m/q/n
grad1.sig.m = pi.m*sig.m/q/n
grad1.pi.s = mult3 + mu.s/q/n
grad1.mu.s = -2*pi.s.old*mult3 + 1/q/n
grad1.sig.s = -4*pi.s.old*sig.s.old*mult3
## terms corresp. to second KL divergence
# update parameters of r(.)
phi.m.sq = clip(sum(pi.m/sig.m^2), ceil=1e3)
phi.s.sq = clip(sum(pi.s/sig.s^2), ceil=1e3)
# calculate gradients
sumZi2 = sum(Z[i,]^2); sumZi = sum(Z[i,])
grad2.pi.m = (sumZi2 - 2*sumZi*mu.m +
q*(mu.m^2 + phi.m.sq))/2/n/sig.m^2
grad2.mu.m = pi.m*(q*mu.m - sumZi)/n/sig.m^2
grad2.sig.m = -pi.m*(sumZi2 - 2*sumZi*mu.m +
q*(mu.m^2 + phi.m.sq))/n/sig.m^3
grad2.pi.s = (sumZi2 - 2*sumZi*mu.s +
q*(mu.s^2 + phi.s.sq))/2/n/sig.s^2
grad2.mu.s = pi.s*(q*mu.s - sumZi)/n/sig.s^2
grad2.sig.s = -pi.s*(sumZi2 - 2*sumZi*mu.s +
q*(mu.s^2 + phi.s.sq))/n/sig.s^3
# update
pi.m = pi.m - eta.epoch*clip(grad.pi.m+grad1.pi.m+grad2.pi.m)
mu.m = mu.m - eta.epoch*clip(grad.mu.m+grad1.mu.m+grad2.mu.m)
sig.m = sig.m - eta.epoch*clip(grad.sig.m+grad1.sig.m+grad2.sig.m)
pi.s = pi.s - eta.epoch*clip(grad.pi.s+grad1.pi.s+grad2.pi.s)
mu.s = mu.s - eta.epoch*clip(grad.mu.s+grad1.mu.s+grad2.mu.s)
sig.s = sig.s - eta.epoch*clip(grad.sig.s+grad1.sig.s+grad2.sig.s)
# normalize parameters
pi.m = abs(pi.m)/sum(abs(pi.m))
pi.s = abs(pi.s)/sum(abs(pi.s))
sig.m = abs(sig.m)
sig.s = abs(sig.s)
}
}
# cat('\t',sig.s,'\n----------\n')
# check convergence
var.diff = sqrt(sum((pi.m.old-pi.m)^2)/sum(pi.m^2)) +
sqrt(sum((pi.s.old-pi.s)^2)/sum(pi.s^2)) +
sqrt(sum((mu.m.old-mu.m)^2)/sum(mu.m^2)) +
sqrt(sum((mu.s.old-mu.s)^2)/sum(mu.s^2)) +
sqrt(sum((sig.m.old-sig.m)^2)/sum(sig.m^2)) +
sqrt(sum((sig.s.old-sig.s)^2)/sum(sig.s^2))
cat("Norm_diff =",round(var.diff,4),sig.s,'\n-----\n')
CONVERGE.var = (var.diff<tol)
if(epoch==max.epoch){
cat("Max iterations reached.",'\n')
break;
}
}
set.seed(07232018)
pi.m = rep(1/K,K)
mu.m = runif(K,-1,1)
sig.m = runif(K,0,1)
pi.s = pi.m
mu.s = runif(K,0,1)
sig.s = runif(K,0,0.1)
Z = GenerateZ(n, q, pi.m, mu.m, sig.m, pi.s, mu.s, sig.s)
sig.s
sig.m
iter = iter + 1
B_old = B_new
Omega_old = Omega_new
Omega.skeleton = which(Omega_old!=0, arr.ind=T)
cat("Iteration ", iter, ":\n")
## Update variational parameters using SGD
cat("Updating variational parameters\n")
epoch = 0
CONVERGE.var=FALSE
set.seed(07232018)
pi.m = rep(1/K,K)
mu.m = runif(K,-1,1)
sig.m = runif(K,0,1)
pi.s = pi.m
mu.s = runif(K,0,1)
sig.s = runif(K,0,0.1)
Z = GenerateZ(n, q, pi.m, mu.m, sig.m, pi.s, mu.s, sig.s)
iter = iter + 1
B_old = B_new
Omega_old = Omega_new
Omega.skeleton = which(Omega_old!=0, arr.ind=T)
cat("Iteration ", iter, ":\n")
## Update variational parameters using SGD
cat("Updating variational parameters\n")
epoch = 0
CONVERGE.var=FALSE
iter = 0; CONVERGE=FALSE; refit.B=TRUE; update.counter=0;
updateTheta = FALSE; # we don't update Theta until B is stabilized a bit
B_new = B_init
Omega_new = Omega_init
Normdiff = rep(0, maxit)
Objval = rep(0, maxit)
B.list = list()
Omega.list = list()
var.list = list()
set.seed(07232018)
pi.m = rep(1/K,K)
mu.m = runif(K,-1,1)
sig.m = runif(K,0,1)
pi.s = pi.m
mu.s = runif(K,0,1)
sig.s = runif(K,0,0.1)
Z = GenerateZ(n, q, pi.m, mu.m, sig.m, pi.s, mu.s, sig.s)
iter = 0; CONVERGE=FALSE; refit.B=TRUE; update.counter=0;
updateTheta = FALSE; # we don't update Theta until B is stabilized a bit
B_new = B_init
Omega_new = Omega_init
Normdiff = rep(0, maxit)
Objval = rep(0, maxit)
B.list = list()
Omega.list = list()
var.list = list()
iter = iter + 1
B_old = B_new
Omega_old = Omega_new
Omega.skeleton = which(Omega_old!=0, arr.ind=T)
cat("Iteration ", iter, ":\n")
## Update variational parameters using SGD
cat("Updating variational parameters\n")
epoch = 0
CONVERGE.var=FALSE
while(!CONVERGE.var){
epoch = epoch+1
eta.epoch = eta*exp(-epoch+1)
perm = gtools::permute(1:n) # randomly order samples
pi.m.old = pi.m
pi.s.old = pi.s
mu.m.old = mu.m
mu.s.old = mu.s
sig.m.old = sig.m
sig.s.old = sig.s
## terms corresp to squared error loss
for(i in perm){ # SGD: go through all samples randomly
xi = X[i,]
for(j0 in 1:nrow(Omega.skeleton)){
# calculate quantities
j = Omega.skeleton[j0,1]
j1 = Omega.skeleton[j0,2]
xj.1.bj1 = xi[j]*sum(B_old[j1,])
bj.t.bj1 = sum(B_old[j,]*B_old[j1,])
mult1 = -(2*Omega_old[j,j1]*xj.1.bj1)/n
# mult2 = exp(clip(2*(mu.s.old + sig.s.old^2), ceil=3))*bj.t.bj1
mult2 = exp(-2*(mu.s + sig.s^2))*bj.t.bj1
# mult2 = exp(2*(mu.s.old + sig.s.old^2))*(bj.t.bj1 +
#                                            q/2/nrow(Omega.skeleton))
# calculatre gradients
grad.pi.m = mult1*mu.m + (mu.m^2+sig.m^2)*bj.t.bj1
grad.mu.m = pi.m*(mult1 + 2*mu.m*bj.t.bj1)
grad.sig.m = 2*sig.m*pi.m*bj.t.bj1
grad.pi.s = mult2
grad.mu.s = -2*mult2*pi.s
grad.sig.s = -4*mult2*pi.s*sig.s
# update
pi.m = pi.m - eta.epoch*clip(grad.pi.m)
mu.m = mu.m - eta.epoch*clip(grad.mu.m)
sig.m = sig.m - eta.epoch*clip(grad.sig.m)
pi.s = pi.s - eta.epoch*clip(grad.pi.s)
mu.s = mu.s - eta.epoch*clip(grad.mu.s)
sig.s = sig.s - eta.epoch*clip(grad.sig.s)
# normalize parameters
pi.m = abs(pi.m)/sum(abs(pi.m))
pi.s = abs(pi.s)/sum(abs(pi.s))
sig.m = abs(sig.m)
sig.s = abs(sig.s)
}
}
# cat('\t',sig.s,'\n----------\n')
## add terms corresp to first KL divergence
mult3 = exp(-2*(mu.s.old + sig.s.old^2))/2
grad1.pi.m = (mu.m.old^2+sig.m.old^2)/2
grad1.mu.m = pi.m.old*mu.m.old
grad1.sig.m = pi.m.old*sig.m.old
grad1.pi.s = mult3 + mu.s.old
grad1.mu.s = -2*pi.s.old*mult3 + 1
grad1.sig.s = -4*pi.s.old*sig.s.old*mult3
# update
pi.m = pi.m - eta.epoch*clip(grad1.pi.m)
mu.m = mu.m - eta.epoch*clip(grad1.mu.m)
sig.m = sig.m - eta.epoch*clip(grad1.sig.m)
pi.s = pi.s - eta.epoch*clip(grad1.pi.s)
mu.s = mu.s - eta.epoch*clip(grad1.mu.s)
sig.s = sig.s - eta.epoch*clip(grad1.sig.s)
## add terms corresp. to second KL divergence
# update parameters of r(.)
phi.m.sq = clip(sum(pi.m/sig.m^2), ceil=1e3)
phi.s.sq = clip(sum(pi.s/sig.s^2), ceil=1e3)
# calculate gradients
sumZ2 = sum(Z^2)/n
sumZ = sum(Z)/n
grad2.pi.m = (sumZ2 - 2*sumZ*mu.m.old +
q*(mu.m.old^2 + phi.m.sq))/2/sig.m.old^2
grad2.mu.m = pi.m.old*(q*mu.m.old - sumZ)/2/sig.m.old^2
grad2.sig.m = -pi.m.old*(sumZ2 - 2*sumZ*mu.m.old +
q*(mu.m.old^2 + phi.m.sq))/sig.m.old^3
grad2.pi.s = (sumZ2 - 2*sumZ*mu.s.old +
q*(mu.s.old^2 + phi.s.sq))/2/sig.s.old^2
grad2.mu.s = pi.s.old*(q*mu.s.old - sumZ)/2/sig.s.old^2
grad2.sig.s = -pi.s.old*(sumZ2 - 2*sumZ*mu.s.old +
q*(mu.s.old^2 + phi.s.sq))/sig.s.old^3
# update
pi.m = pi.m - eta.epoch*clip(grad2.pi.m)
mu.m = mu.m - eta.epoch*clip(grad2.mu.m)
sig.m = sig.m - eta.epoch*clip(grad2.sig.m)
pi.s = pi.s - eta.epoch*clip(grad2.pi.s)
mu.s = mu.s - eta.epoch*clip(grad2.mu.s)
sig.s = sig.s - eta.epoch*clip(grad2.sig.s)
# normalize parameters
pi.m = abs(pi.m)/sum(abs(pi.m))
pi.s = abs(pi.s)/sum(abs(pi.s))
sig.m = abs(sig.m)
sig.s = abs(sig.s)
# check convergence
var.diff = sqrt(sum((pi.m.old-pi.m)^2)/sum(pi.m^2)) +
sqrt(sum((pi.s.old-pi.s)^2)/sum(pi.s^2)) +
sqrt(sum((mu.m.old-mu.m)^2)/sum(mu.m^2)) +
sqrt(sum((mu.s.old-mu.s)^2)/sum(mu.s^2)) +
sqrt(sum((sig.m.old-sig.m)^2)/sum(sig.m^2)) +
sqrt(sum((sig.s.old-sig.s)^2)/sum(sig.s^2))
cat("Norm_diff =",round(var.diff,4),sig.s,'\n-----\n')
CONVERGE.var = (var.diff<tol)
if(epoch==max.epoch){
cat("Max iterations reached.",'\n')
break;
}
}
grad.pi.m
grad.pi.s
grad.mu.m
grad.mu.s
grad1.mu.m
grad1.mu.s
grad1.pi.m
grad1.pi.s
grad2.pi.m
grad2.pi.s
